{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4074788b-d26e-443c-a91c-89af4bdfd036",
   "metadata": {},
   "source": [
    "# 1 - Business Opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a970093-e3ca-4d26-82fd-121f83afc733",
   "metadata": {},
   "source": [
    "## The Project Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f35aa6-bf7b-4f7b-94db-87a2cd6f59f7",
   "metadata": {},
   "source": [
    "AAVAIL aims to transition from a tiered subscription model to an à la carte service model based on user feedback and evolving market demands, especially in international markets.   \n",
    "In order to achieve this, the company seeks to develop a revenue projection tool that can accurately predict monthly revenue for specific countries using a machine learning model.     \n",
    "This tool is crucial for the management team, who currently rely on manual methods to estimate revenue and face challenges with accuracy and efficiency.    \n",
    "By automating revenue predictions, AAVAIL hopes to streamline decision-making processes, stabilize staffing and budget projections, and ultimately enhance business performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd49da8-9a86-42f5-8604-6acbe5a35890",
   "metadata": {},
   "source": [
    "## The Stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941de858-16ae-4c00-ad08-a3a444478c07",
   "metadata": {},
   "source": [
    "Two primary groups have been involved in the design thinking process:\n",
    "\n",
    "- `End Users`: AAVAIL customers who will benefit from the new subscription model.\n",
    "- `Managers`: AAVAIL responsibles for revenue projections and who have expressed a need for a more effective forecasting tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4db28-e132-4e1f-96bd-4b18c6276e40",
   "metadata": {},
   "source": [
    "## The Ideal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a5593-1e17-41e7-a71f-0221dd7e485d",
   "metadata": {},
   "source": [
    "To effectively address the business opportunity, the following data points would be ideal:\n",
    "\n",
    "**Transaction-Level Data:**\n",
    "- `Date`: The date of each transaction to analyze revenue trends over time.\n",
    "- `Invoice ID`: A unique identifier for each transaction, stripped of non-numeric characters for consistency.\n",
    "- `Service Type`: The specific service subscribed to, allowing for differentiation between offerings.\n",
    "- `Times viewed` : The number of times a particular content or service was viewed by the customer, providing insights into engagement levels.\n",
    "- `Transaction Amount`: The revenue generated from each transaction to calculate total revenue.\n",
    "\n",
    "**User Data:**                            \n",
    "- `User ID`: A unique identifier for each user to track their purchasing behavior.\n",
    "- `Subscription Type`: Details about whether users are on à la carte or tiered subscriptions.\n",
    "- `User Demographics`: Age, gender, and other demographics to analyze trends in purchasing behavior.\n",
    "\n",
    "**Country Data**\n",
    "- `Country`: The country of each user to analyze revenue generation across different markets.\n",
    "- `City` : The city of each user to analyze revenue generation across different local markets.\n",
    "- `GDP per Capita` : The total economic output of the country, often used as a measure of economic health and consumer spending potential.\n",
    "- `Currency Exchange Rates` : The current value of the country's currency relative to other major currencies, particularly USD or EUR, which could affect subscription pricing.\n",
    "- `Holidays and Seasonal Trends`: National holidays or cultural events that could impact subscription patterns or usage trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8a63f-a59f-4123-a232-fa9b976050a5",
   "metadata": {},
   "source": [
    "## The Available Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173902ff-bb15-4bf1-9305-fc3acc24de92",
   "metadata": {},
   "source": [
    "There is a dataset consisting of transaction-level purchases from several thousand active users across 38 countries, providing valuable historical data. The fields of this dataset are as follow :\n",
    "\n",
    "- `date` : The date in which the data was recorded, indicating when the transactions occurred.\n",
    "- `country` : The country where the transaction took place, providing geographical context for the sales data.\n",
    "- `invoice` : A unique identifier for each transaction or purchase, enabling tracking of individual sales.\n",
    "- `customer_id` : A unique identifier for each customer, allowing for tracking and analysis of customer behavior and purchase patterns.\n",
    "- `stream_id` : An identifier for the content or service that was accessed or purchased, indicating the source of the transaction.\n",
    "- `times_viewed` : The number of times a particular content or service was viewed by the customer, providing insights into engagement levels.\n",
    "- `price` : The monetary amount charged for the transaction, reflecting the revenue generated from each sale.\n",
    "\n",
    "This dataset was supplied by the company as multiple `JSON` files, each representing a month of the year. \n",
    "\n",
    "Each file contains a `list of dictionaries` featuring the specified key fields mentioned above and historical data values spanning `from November 2017 to July 2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bda638-51cb-4349-b460-a3fca610c6df",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb8bcf-2cd1-46cb-ba3e-59acd88580d9",
   "metadata": {},
   "source": [
    "# 2 - Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ff3bc-a5cd-4619-a6d8-9a83178066f4",
   "metadata": {},
   "source": [
    "In this section, we are creating a python script to `extract the relevant data` from multiple data sources, automating the process of data ingestion.\n",
    "From within a Python module the function `XXX` reads in the data, attempts to catch common input errors and returns a feature matrix that will subsequently be used as a starting point for the EDA and the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6e659-8100-437c-9ad0-b1974c5ddfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary librairies \n",
    "\n",
    "from ingestion_module import load_json_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import probplot\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools  # Import itertools for combinations\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print('\\nNecessary librairies imported\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c159b0-cb8a-40f7-8814-8e9a102cdbda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## loading the dataframe as loaded_df\n",
    "\n",
    "loaded_df = load_json_data('cs-train')\n",
    "\n",
    "print(\"\\n... Dataframe loaded as 'loaded_df'\\n\")\n",
    "print('The columns are', loaded_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36bda7-2e7a-4acb-bf52-02ebbac329f7",
   "metadata": {},
   "source": [
    "## A - Basic Data Exploratory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306ccf6-4756-4518-b96f-70bfee0cd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Duplicate rows in loaded_df\n",
    "\n",
    "duplicate_rows = loaded_df.duplicated(keep=\"first\")\n",
    "total_duplicates = duplicate_rows.sum()\n",
    "print(f\"\\nWarning: There are {total_duplicates} duplicate rows\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2759c0-d262-4c7d-908b-6b2db7fe881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the loaded_df named loaded_df_original\n",
    "\n",
    "loaded_df_original = loaded_df.copy()  \n",
    "\n",
    "print(\"\\nA deep copy of loaded_df named loaded_df_original has been created'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42fca3-209e-412e-983e-046b15bac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicate rows in loaded_df\n",
    "\n",
    "print(\"\\nDuplicated Summary\\n{}\".format(\"-\"*22))\n",
    "size_before = len(loaded_df)\n",
    "loaded_df.drop_duplicates(keep=\"first\", inplace=True )\n",
    "size_after = len(loaded_df)\n",
    "print(\"... removed {} duplicate rows in the loaded_df\\n\".format(size_before-size_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122279a-8c60-44aa-a01c-1d3c71fa497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns\n",
    "numerical_cols = ['times_viewed','price']\n",
    "print('the numerical columns in loaded_df are\\n',numerical_cols)\n",
    "\n",
    "print(\"\\nThe loaded_df total rows before the outlier detection is {:,.0f}\".format(loaded_df.shape[0]), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d37462-3916-4cb3-93d0-75a8175873c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot distribution and check for normality\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plotting the histogram and KDE for each numerical column\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(loaded_df[col], kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(loaded_df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot of {col}')\n",
    "    \n",
    "    # Shapiro-Wilk Test for normality\n",
    "    stat, p_value = stats.shapiro(loaded_df[col].dropna())\n",
    "    \n",
    "    # Check if p-value is significant (typically less than 0.05 means non-normal distribution)\n",
    "    if p_value > 0.05:\n",
    "        print(f'{col} looks normally distributed (p-value: {p_value:.3f})')\n",
    "    else:\n",
    "        print(f'{col} is not normally distributed (p-value: {p_value:.3f})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99074ec0-e204-44b1-a647-4a1a27eb282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Outliers in loaded_df\n",
    "\n",
    "# Function to find outliers based on IQR for a single column within each country\n",
    "def find_outliers_index_iqr_by_country(df):\n",
    "    outliers_index = []  # To store the indices of rows with outliers\n",
    "\n",
    "    # Iterate over each country in the DataFrame\n",
    "    for country, group in df.groupby('country'):\n",
    "        for column in numerical_cols:  \n",
    "            Q1 = group[column].quantile(0.25)\n",
    "            Q3 = group[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Calculate the lower and upper bounds for outliers\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Find indices of outliers in the column\n",
    "            outliers_in_column = group[(group[column] < lower_bound) | (group[column] > upper_bound)].index\n",
    "            \n",
    "            # Extend the outliers_index list with these indices\n",
    "            outliers_index.extend(outliers_in_column)\n",
    "    \n",
    "    # Return the outliers and the cleaned DataFrame\n",
    "    return outliers_index\n",
    "\n",
    "\n",
    "# Call the function to detect outliers\n",
    "outliers_index_of_loaded_df = find_outliers_index_iqr_by_country(loaded_df)\n",
    "#loaded_df.loc[outliers_index_of_loaded_df]\n",
    "\n",
    "# Display the outliers\n",
    "print(\"Warning : Outliers detected\")\n",
    "print(\"There are {:,.0f} rows with outliers\".format(loaded_df.loc[outliers_index_of_loaded_df].shape[0]))\n",
    "print(f\"{(100 * loaded_df.loc[outliers_index_of_loaded_df].shape[0] / loaded_df.shape[0]):.0f}% of the data seems to be outliers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632ba15-26e6-4bb0-8ce2-884152f1323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain outliers and return the cleaned DataFrame\n",
    "cleaned_from_outliers_loaded_df = loaded_df.drop(index=outliers_index_of_loaded_df)\n",
    "\n",
    "# Display the cleaned data shape\n",
    "print(\"\\nThe loaded_df's total rows before the outlier detection is {:,.0f}\".format(loaded_df.shape[0]), \"rows\")\n",
    "print(\"\\nThe loaded_df's total rows after the outlier detection is {:,.0f}\".format(cleaned_from_outliers_loaded_df.shape[0]), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164b8a2-7cf6-40ba-b506-201b0e756b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution and check for normality after droping outliers\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plotting the histogram and KDE for each numerical column\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(cleaned_from_outliers_loaded_df[col], kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(cleaned_from_outliers_loaded_df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot of {col}')\n",
    "    \n",
    "    # Shapiro-Wilk Test for normality\n",
    "    stat, p_value = stats.shapiro(cleaned_from_outliers_loaded_df[col].dropna())\n",
    "    \n",
    "    # Check if p-value is significant (typically less than 0.05 means non-normal distribution)\n",
    "    if p_value > 0.05:\n",
    "        print(f'After droping outliers, {col} looks normally distributed (p-value: {p_value:.3f})')\n",
    "    else:\n",
    "        print(f'After droping outliers, {col} is not normally distributed (p-value: {p_value:.3f})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c1428-7d60-47f2-bd94-6417b93c4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Function to find outliers based on Isolation Forest for numerical columns within each country\n",
    "def find_outliers_index_isolation_forest_by_country(df):\n",
    "    outliers_index = []  \n",
    "\n",
    "    # Iterate over each country in the DataFrame\n",
    "    for country, group in df.groupby('country'):\n",
    "        # Apply IsolationForest for the selected numerical columns\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        \n",
    "        # Fit the model and predict outliers (returns -1 for outliers, 1 for inliers)\n",
    "        outliers = iso_forest.fit_predict(group[numerical_cols])\n",
    "\n",
    "        # Find indices of outliers (where prediction is -1)\n",
    "        outliers_in_group = group[outliers == -1].index\n",
    "        \n",
    "        # Extend the outliers_index list with these indices\n",
    "        outliers_index.extend(outliers_in_group)\n",
    "    \n",
    "    # Return the outliers' indices\n",
    "    return outliers_index\n",
    "\n",
    "# Call the function to detect outliers\n",
    "outliers_index_IsolationForest_of_loaded_df = find_outliers_index_isolation_forest_by_country(loaded_df)\n",
    "\n",
    "# Display the outliers\n",
    "print(\"Warning : Outliers detected\")\n",
    "print(\"There are {:,.0f} rows with outliers\".format(loaded_df.loc[outliers_index_IsolationForest_of_loaded_df].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d395e-ffef-4922-990c-e9d757af9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain outliers and return the cleaned DataFrame\n",
    "cleaned_from_IsolationForest_outliers_loaded_df = loaded_df.drop(index=outliers_index_IsolationForest_of_loaded_df)\n",
    "\n",
    "# Display the cleaned data shape\n",
    "print(\"\\nThe loaded_df total rows before the outlier detection is {:,.0f}\".format(loaded_df.shape[0]), \"rows\")\n",
    "print(\"\\nThe loaded_df total rows after the outlier detection is {:,.0f}\".format(cleaned_from_IsolationForest_outliers_loaded_df.shape[0]), \"rows\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b60f7a-82ea-4c9f-9925-9dd455d5d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution and check for normality after droping outliers\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plotting the histogram and KDE for each numerical column\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(cleaned_from_IsolationForest_outliers_loaded_df[col], kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(cleaned_from_IsolationForest_outliers_loaded_df[col], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot of {col}')\n",
    "    \n",
    "    # Shapiro-Wilk Test for normality\n",
    "    stat, p_value = stats.shapiro(cleaned_from_IsolationForest_outliers_loaded_df[col].dropna())\n",
    "    \n",
    "    # Check if p-value is significant (typically less than 0.05 means non-normal distribution)\n",
    "    if p_value > 0.05:\n",
    "        print(f'After droping outliers, {col} looks normally distributed (p-value: {p_value:.3f})')\n",
    "    else:\n",
    "        print(f'After droping outliers, {col} is not normally distributed (p-value: {p_value:.3f})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb68bd3-6019-498e-82af-ab53c24348a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Missing Values in loaded_df\n",
    "\n",
    "missing_values = loaded_df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nWarning: There are some missing values in the dataframe\")\n",
    "    print(\"\\nMissing_values Summary\\n{}\".format(\"-\"*25))\n",
    "else :\n",
    "    print(\"\\nThere are no missing values in the dataframe\")\n",
    "    \n",
    "# Filter for columns with missing values\n",
    "missing_data = missing_values[missing_values > 0]\n",
    "\n",
    "# Print the column name and number of missing values\n",
    "for column, missing_count in missing_data.items():\n",
    "    print(f\"There are {missing_count} missing values in the {column} column\\n\\nBut since we are not using the {column} column in our analysis, we won't drop rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67f336-816c-4ac8-9525-fcf805242c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for uniqueness of entries in each column of loaded_df\n",
    "\n",
    "print(\"\\nUniqueness of entries in each column\\n{}\".format(\"-\"*39))\n",
    "for column in ['country','date','invoice_id','customer_id','stream_id'] :\n",
    "    num_unique = cleaned_from_IsolationForest_outliers_loaded_df[column].nunique()           \n",
    "    \n",
    "    print(f\"There are {num_unique} unique {column}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737db25-d15e-47d1-aded-7e90c51499ae",
   "metadata": {},
   "source": [
    "## B - Dates investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066e165-239c-4f7d-9635-a117c951b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dates Span and Missing Observations\n",
    "\n",
    "max_date = cleaned_from_IsolationForest_outliers_loaded_df.date.max()\n",
    "min_date = cleaned_from_IsolationForest_outliers_loaded_df.date.min()\n",
    "\n",
    "span_dates = (max_date -  min_date).days + 1\n",
    "nunique_dates = cleaned_from_IsolationForest_outliers_loaded_df.date.nunique()\n",
    "missing_observations = span_dates - nunique_dates\n",
    "\n",
    "\n",
    "print(f'\\nAs highlighted above, there are {nunique_dates} unique date, but the dates span a range of {span_dates} days \\\n",
    "- starting the {min_date} and ending the {max_date} - giving us {missing_observations} missing observations.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8608e-01e7-4116-9538-90a7321953e1",
   "metadata": {},
   "source": [
    "## C - Revenu investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab05bc-c74a-4e13-9374-bab7c234a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and calculate total revenue for each country\n",
    "revenue_country = (cleaned_from_IsolationForest_outliers_loaded_df[['country', 'price']]\n",
    "                   .groupby('country', as_index=False)\n",
    "                   .sum()\n",
    "                   .rename(columns={'price': 'revenue_'})\n",
    "                  ).sort_values(by='revenue_', ascending=False).reset_index().head(5)\n",
    "\n",
    "# Calculate total revenue and percentage contribution for each country, then get the top 5\n",
    "revenue_country_ = (revenue_country\n",
    "                   .assign(percentage=lambda df: (df['revenue_'] / df['revenue_'].sum()) * 100)\n",
    "                   .assign(revenue=lambda df: df['revenue_'].apply(lambda x: f\"{x:,.2f}\"),\n",
    "                           percentage=lambda df: df['percentage'].apply(lambda x: f\"{x:.2f}%\"))\n",
    ")\n",
    "\n",
    "# Display the top five revenue-generating countries\n",
    "print('\\nThe top five revenue-generating countries\\n')\n",
    "revenue_country_[['country', 'revenue', 'percentage']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9549f-a63e-4f24-9c37-2781d2930782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffbb09-70a9-4eb9-9e2e-04d8f32e4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a bar plot for the top 5 revenue-generating countries\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=revenue_country.revenue_.tolist(), \n",
    "            y=revenue_country.country.tolist(), \n",
    "            data=revenue_country, \n",
    "            palette='viridis')\n",
    "\n",
    "# use a log scale\n",
    "plt.xscale('log') \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Revenue')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Top 5 Revenue-Generating Countries')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b4491-da4d-4f05-9723-8f33833a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate revenue ratio\n",
    "revenue_ratio = (revenue_country['revenue_'].iloc[0] /\n",
    "                 revenue_country['revenue_'].iloc[1:].sum()).round(0)\n",
    "print(f'\\nThe revenue generated by {revenue_country[\"country\"].iloc[0]} is approximately '\n",
    "      f'{revenue_ratio} times greater than the combined revenue of the next four countries.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f411e-53de-418f-b283-0858cac9832a",
   "metadata": {},
   "source": [
    "## C - Streams investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbd987-a26f-4dea-b6b2-85e9871e641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343eca4f-9513-4b29-b79a-dae36a46091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identifier les stream_id les plus fréquents\n",
    "stream_id_counts = cleaned_from_IsolationForest_outliers_loaded_df['stream_id'].value_counts()\n",
    "most_frequent_streams = stream_id_counts[stream_id_counts > 1999]\n",
    "print(\"\\nLes stream_id les plus fréquents :\")\n",
    "print(most_frequent_streams)\n",
    "\n",
    "# 4. Analyser la relation entre stream_id et times_viewed\n",
    "stream_times_viewed = cleaned_from_IsolationForest_outliers_loaded_df.groupby('stream_id')['times_viewed'].mean().sort_values(ascending=False)\n",
    "print(\"\\nNombre moyen de fois qu'un stream a été vu :\")\n",
    "print(stream_times_viewed)\n",
    "\n",
    "# 5. Analyser la relation entre stream_id et le prix\n",
    "stream_price = cleaned_from_IsolationForest_outliers_loaded_df.groupby('stream_id')['price'].mean().sort_values(ascending=False)\n",
    "print(\"\\nPrix moyen par stream_id :\")\n",
    "print(stream_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e4905-01e1-4660-8feb-8cc18138ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba14c5-2c8c-4115-8981-254a78a46e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66c06b-89d6-463c-8e01-46f09ccb8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f2095-9d9a-40c8-b868-9335a64085a9",
   "metadata": {},
   "source": [
    "# 3 - Testable Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a025a-7640-4c99-9381-f98bc284e75a",
   "metadata": {},
   "source": [
    "In order to derive meaningful insights from the dataset, we propose a series of testable hypotheses that explore the relationships between various factors influencing revenue generation. \n",
    "These hypotheses aim to identify key trends and patterns in user behavior, seasonal effects, and the effectiveness of different subscription models, ultimately guiding strategic decision-making for this business opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe310d-4292-475d-ba5c-b83a4e85c94f",
   "metadata": {},
   "source": [
    "### Hypothesis 1: Revenue Prediction by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b832c-8d7a-4df1-9f44-6c6679c223d4",
   "metadata": {},
   "source": [
    "`Statement` : The monthly revenue from the top five countries is significantly different from each other.\n",
    "\n",
    "`Involved Fields` : country, price, year-month\n",
    "\n",
    "`Target` : The Monthly Revenue which is the total revenue generated in a given month for each country.\n",
    "\n",
    "`Business Metric` : The Revenue Variance Across Countries: The key business metric to be evaluated is whether the monthly revenue varies significantly across the top ten countries. This is critical for understanding market performance and tailoring the company's revenue generation strategies accordingly.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "`Data Aggregation`: Aggregate the data by country, year, and month to calculate the total monthly revenue for each country. This can be achieved by summing the price for each combination of country, year, and month.\n",
    "The resulting dataset will have fields: country, year, month, and monthly_revenue.\n",
    "\n",
    "`Statistical Analysis`:\n",
    "- Statistical tests (e.g., ANOVA) : To compare the monthly revenues across the top five countries. This will help determine if there are statistically significant differences in revenue generation.\n",
    "- Calculate summary statistics (mean, median, variance) for monthly revenue in each country to assess overall performance and variability.\n",
    "\n",
    "`Visualizations` : Depict the distribution of monthly revenue across the top ten countries. This will help in identifying outliers and trends visually.  \n",
    "Time series plots can also be beneficial to show trends over time for each country, highlighting seasonal patterns or anomalies.\n",
    "\n",
    "`Correlation Analysis`: Explore correlations between monthly revenue and other factors such as times_viewed, if applicable. This could provide insights into whether user engagement impacts revenue generation.\n",
    "\n",
    "`Insights Extraction`:   \n",
    "Based on the results of the statistical tests and visualizations, draw conclusions about the revenue generation capabilities of each country. Identify which countries are performing well and which are underperforming.\n",
    "Assess whether external factors (e.g., economic conditions, cultural events) correlate with the observed revenue patterns, which could inform strategic decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d4676-b741-4dd3-b9b0-92281f58339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion_module import time_series_df\n",
    "\n",
    "# Working only with the The top five revenue-generating countries\n",
    "countries = revenue_country.country.tolist()\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "violin_df = None\n",
    "\n",
    "# Combining the DataFrames for each country into a single DataFrame\n",
    "for country in countries:\n",
    "    # Creating an concatenated df \n",
    "    df = time_series_df(cleaned_from_IsolationForest_outliers_loaded_df, country=country)\n",
    "    df['country'] = country\n",
    "    # Concatenate all DataFrames into one\n",
    "    if violin_df is None:\n",
    "        violin_df = df.copy()\n",
    "    else:\n",
    "        violin_df = pd.concat([violin_df, df], ignore_index=True)  \n",
    "\n",
    "# Plotting the violinplot\n",
    "sns.violinplot(x='country', y='revenue', data=violin_df)  # Remove scale='log'\n",
    "plt.yscale('log')  # Set y-axis to log scale\n",
    "plt.title('Boxplot of Daily Log Revenues\\nfor Each Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Daily Log Revenue')  # Update label to reflect log scale\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055ed89-5965-41b0-8e35-9d0e49569090",
   "metadata": {},
   "source": [
    "Overall, the violin plot suggests that the United Kingdom is the top revenue-generating country with a higher median revenue and a wider range of daily revenue. The other countries have similar distributions of daily revenue, with a median close to 0 and a smaller spread.\n",
    "\n",
    "Based on these observations, we might expect to find significant differences in mean revenues among the countries, especially between the United Kingdom and the other countries. However, ANOVA analysis would be necessary to confirm this statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fcfee-cb31-48a0-8fea-6e1cf9e3c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion_module import time_series_df\n",
    "\n",
    "countries = revenue_country.country.tolist()\n",
    "anova_long_df = None\n",
    "\n",
    "for country in countries:\n",
    "     # Adding the country name as a prefix to each country revenue column in the below df\n",
    "    revenue_column = country.replace(\" \", \"_\") + \"_revenue\"\n",
    "    # Creating an aggregated df by year-month and monthly revenue for each country.\n",
    "    df = (time_series_df(cleaned_from_IsolationForest_outliers_loaded_df, country=country)\n",
    "          .groupby('year-month')\n",
    "          .agg(sum_revenue = ('revenue', 'sum') )\n",
    "          .round(2)\n",
    "          .reset_index()\n",
    "         ) \n",
    "    df['country'] = country\n",
    "\n",
    "    if anova_long_df is None:\n",
    "        anova_long_df = df.copy()\n",
    "    else:\n",
    "        anova_long_df = pd.concat([anova_long_df, df], ignore_index=True)  \n",
    "\n",
    "anova_long_df = anova_long_df.fillna(0)\n",
    "anova_long_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94cd9b-9139-4a0e-be1b-5b040521a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Group by country\n",
    "for country, group in anova_long_df.groupby('country'):\n",
    "    print(f\"\\nNormality test of sum_revenue data for {country}:\")\n",
    "    \n",
    "    # Step 2: Perform the Shapiro-Wilk test on sum_revenue for each country\n",
    "    stat, p_value = shapiro(group['sum_revenue'])\n",
    "    \n",
    "    # Step 3: Print the test results\n",
    "    print(f\"Shapiro-Wilk test statistic: {stat}, p-value: {round(p_value,2)}, Sample Size: {group.shape[0]}\")\n",
    "    \n",
    "    # Interpretation of the p-value\n",
    "    if p_value > 0.05:\n",
    "        print(f\"\\nThe sum_revenue for {country} appears to be normally distributed (fail to reject H0)\\n\")\n",
    "    else:\n",
    "        print(f\"\\nThe sum_revenue for {country} does not appear to be normally distributed (reject H0)\\n\\\n",
    "But since the sample size is greater that 20, the CLT helps to mitigate normality concerns.\")\n",
    "    \n",
    "    # Step 4: Plot the distribution of sum_revenue for visual inspection\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(group['sum_revenue'], kde=True)\n",
    "    plt.title(f'{country} - Histogram of sum_revenue')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(group['sum_revenue'], dist=\"norm\", plot=plt)\n",
    "    plt.title(f'{country} - Q-Q Plot of sum_revenue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c73f31-8159-4d6a-a6e9-78f828393957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = [group['sum_revenue'].values for name, group in anova_long_df.groupby('country')]\n",
    "\n",
    "# Perform Levene's test\n",
    "stat, p_value = levene(*groups)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Levene's Test statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value > 0.05:\n",
    "    print(\"The variances across countries are approximately equal (fail to reject H0).\")\n",
    "else:\n",
    "    print(\"The variances across countries are significantly different (reject H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f104d-01cf-4d76-8566-d0044583d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by country\n",
    "groups = [group['sum_revenue'].values for name, group in anova_long_df.groupby('country')]\n",
    "\n",
    "# Perform Welch's ANOVA\n",
    "def welch_anova(*groups):\n",
    "    k = len(groups)  # number of groups\n",
    "    # Calculate the means and variances for each group\n",
    "    means = [np.mean(g) for g in groups]\n",
    "    vars = [np.var(g, ddof=1) for g in groups]  # sample variance\n",
    "    ns = [len(g) for g in groups]  # sample sizes\n",
    "\n",
    "    # Calculate the Welch's ANOVA statistic\n",
    "    num = sum(n * (mean - np.mean(means))**2 for n, mean in zip(ns, means))\n",
    "    denom = sum((n - 1) * var for n, var in zip(ns, vars))\n",
    "\n",
    "    F = num / denom\n",
    "    return F\n",
    "\n",
    "# Calculate F-statistic\n",
    "F_stat = welch_anova(*groups)\n",
    "\n",
    "# Calculate p-value using the F-distribution\n",
    "dfn = len(groups) - 1  # numerator degrees of freedom\n",
    "dfd = sum(len(g) - 1 for g in groups)  # denominator degrees of freedom\n",
    "p_value = 1 - f.cdf(F_stat, dfn, dfd)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Welch's ANOVA F-statistic: {round(F_stat,4)}, p-value: {round(p_value,4)}\")\n",
    "\n",
    "# Interpretation of results\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There are significant differences in sum_revenue among countries.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant differences in sum_revenue among countries.\")\n",
    "    \n",
    "# Further interpretation\n",
    "if p_value < alpha:\n",
    "    print(\"Post-hoc tests (e.g., Tukey's HSD) can be conducted to identify which countries have significant differences in revenue.\")\n",
    "else:\n",
    "    print(\"Since there are no significant differences, post-hoc tests are not necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbc237-0729-4c63-8609-01b3a683462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Fit the model\n",
    "model = ols('sum_revenue ~ C(country)', data=anova_long_df).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Displaying the results\n",
    "print('\\nANOVA TABLE\\n{}'.format(\"-\"*59))\n",
    "display(anova_table)\n",
    "print((\"-\"*59))\n",
    "# Interpretation\n",
    "if anova_table['PR(>F)'][0] < 0.05:\n",
    "    print(\"\\nWe reject the null hypothesis and accept the alternative one stating that :\\nThere are significant differences in monthly revenue generation across countries.\\n\")\n",
    "else:\n",
    "    print(\"\\nWe fail to reject the null hypothesis stating that :\\nThere are no significant differences in monthly revenue generation across countries.\\n\")\n",
    "# Further interpretation\n",
    "if p_value < alpha:\n",
    "    print(\"Post-hoc tests (e.g., Tukey's HSD) can be conducted to identify which countries have significant differences in revenue.\")\n",
    "else:\n",
    "    print(\"Since there are no significant differences, post-hoc tests are not necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5151b09-19aa-46c5-b107-8f76f72dcad7",
   "metadata": {},
   "source": [
    "##### Let's dive a little deeper into the analysis and perform Tukey's HSD (Honestly Significant Difference) test which is a popular choice\n",
    "#####  for pairwise comparisons identifying which specific country differ significantly from each other after the ANOVA result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fe11c-b696-46cc-aead-f3f247f699c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.multicomp as smc\n",
    "\n",
    "# Perform Tukey's HSD Test\n",
    "tukey_results = smc.pairwise_tukeyhsd(anova_long_df['sum_revenue'], anova_long_df['country'])\n",
    "print('\\n', tukey_results, '\\n')\n",
    "\n",
    "# Interpretation of Tukey's HSD results\n",
    "print(\"\\nTukey's HSD Results Interpretation:\\n\")\n",
    "for idx, row in enumerate(tukey_results.summary()):\n",
    "    if idx == 0:  # skip the header row\n",
    "        continue\n",
    "    group1, group2, meandiff, p_adj, lower, upper, reject = map(str, row)  # Convert each cell to string\n",
    "    meandiff = float(meandiff)  # Convert mean difference to float\n",
    "    p_adj = float(p_adj)  # Convert adjusted p-value to float\n",
    "\n",
    "    if reject == 'True':  # Check if reject is a string representation of boolean\n",
    "        print(f\"There is a significant difference between {group1} and {group2}\")\n",
    "    else:\n",
    "        print(f\"There is no significant difference between {group1} and {group2}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658e6e0-4e14-470f-9c4d-3be47f5f9989",
   "metadata": {},
   "source": [
    "The Tukey HSD test confirms that the United Kingdom has significantly higher mean revenue compared to the other countries.  \n",
    "\n",
    "\n",
    "However, there are no significant differences in mean revenue among the other four countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ecf5c2-ca75-4335-9980-dfb5f6a4ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45d5e4-3b7b-41b5-8222-ae92dc4f69f2",
   "metadata": {},
   "source": [
    "### Hypothesis 2: Impact of User Engagement on Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d60a2-3f42-4da2-9c2e-d9b65df1dd92",
   "metadata": {},
   "source": [
    "`Statement`: Higher times viewed of a service correlates with increased revenue generated from that service.\n",
    "\n",
    "``Involved Fields``: times_viewed, price, year, month, country\n",
    "\n",
    "`Target` : The Monthly Revenue which is the total revenue generated in a given month for each country.\n",
    "\n",
    "`Business Metric` : The Correlation Between User Engagement and Revenue: The key business metric to evaluate is the relationship between times_viewed and monthly revenue. Understanding this relationship is essential for optimizing marketing efforts and improving revenue generation strategies.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "`Data Aggregation`: Aggregate the data by country, year, month, and potentially by service type (if available) to calculate the total monthly revenue and total times viewed for each service.\n",
    "The resulting dataset will have fields: country, year, month, total_times_viewed, and monthly_revenue.\n",
    "\n",
    "`Statistical Analysis` : \n",
    "- Use correlation coefficients (e.g., Pearson or Spearman correlation) to quantify the relationship between total_times_viewed and monthly revenue. A positive correlation would indicate that higher views are associated with increased revenue.\n",
    "- Conduct regression analysis (e.g., linear regression) to model the relationship between times_viewed and monthly revenue, controlling for other factors such as country, year, and month. This can provide insights into how much of the variance in revenue can be explained by user engagement.\n",
    "\n",
    "`Visualizations`: Create scatter plots to visualize the relationship between total_times_viewed and monthly revenue. Adding a trend line can help illustrate any potential correlation.\n",
    "Box plots can be useful to show the distribution of monthly revenue for different ranges of times_viewed, highlighting potential outliers and the central tendency.\n",
    "\n",
    "`Insights Extraction`: Based on the results of the correlation and regression analyses, draw conclusions about the strength and nature of the relationship between user engagement (measured by times_viewed) and revenue generation.\n",
    "Identify whether increasing user engagement strategies (like improved marketing or user experience) could potentially lead to higher revenues.\n",
    "\n",
    "`Considerations`: It’s important to control for other confounding variables that might influence both times_viewed and monthly revenue. Factors such as seasonality, pricing changes, or marketing campaigns could be included in a multivariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd619012-f4ba-460e-88f3-af5ebc392bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion_module import time_series_df\n",
    "\n",
    "time_series_cleaned_from_IsolationForest_outliers_loaded_df = time_series_df(cleaned_from_IsolationForest_outliers_loaded_df, country=None)\n",
    "display(time_series_cleaned_from_IsolationForest_outliers_loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923d92e-e92c-4c4c-8ace-caeaae47ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df = (time_series_cleaned_from_IsolationForest_outliers_loaded_df\n",
    "                                                                       .groupby('year-month')\n",
    "                                                                       .sum(numeric_only=True)\n",
    "                                                                       .reset_index()\n",
    "                                                                      )\n",
    "\n",
    "grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8eb03a-e71f-4397-8792-5ee0580bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for normality\n",
    "def check_normality(df):\n",
    "    for column in df.select_dtypes(include=[np.number]).columns:\n",
    "        # Shapiro-Wilk Test\n",
    "        stat, p = shapiro(df[column])\n",
    "        print(f'\\nShapiro-Wilk test for {column}: statistic = {stat:.3f}, p-value = {p:.3f}')\n",
    "        # Interprettaions\n",
    "        if p < 0.05:\n",
    "            print(f'-> {column} are not normally distributed (reject H0)\\n')\n",
    "        else:\n",
    "            print(f'-> {column} are normally distributed (fail to reject H0)\\n')\n",
    "\n",
    "            \n",
    "        # Plot the distribution of the column for visual inspection\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "    \n",
    "        # Q-Q plot for normality check\n",
    "        plt.subplot(1, 2, 2)\n",
    "        stats.probplot(df[column], dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot of {column}')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Check normality\n",
    "\n",
    "check_normality(grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bf679-fe02-4474-9f3f-c2b0467e366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "corr_matrix_spearman = grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df[numerical_columns].corr(method='spearman')\n",
    "\n",
    "fig , ax = plt.subplots(figsize=(7,7))\n",
    "mask = np.triu(np.ones_like(corr_matrix_spearman))\n",
    "ax=sns.heatmap(corr_matrix_spearman,\n",
    "               annot=True, fmt='.2f', ax=ax , linewidth=0.5,\n",
    "               vmin=-1, vmax=1, mask=mask, cmap='BrBG', cbar=True)\n",
    "ax.set_title('The correlation heatmap');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca6114-51c3-447e-a0ff-18bc34c94291",
   "metadata": {},
   "source": [
    "The data indicates that user engagement metrics (purchases, streams, and views) are related to one another, and have a direct impact on revenue.\n",
    "\n",
    "\n",
    "Customer Engagement: The strong correlations between unique streams, total views, and revenue suggest that increasing customer engagement through more content views and unique streams is a key driver of revenue growth.\n",
    "\n",
    "\n",
    "Sales Funnel: The relationship between unique invoices and the other variables indicates that while unique invoices are important, they may not be the sole determinant of revenue. Factors like customer engagement and content consumption also play significant roles.\n",
    "\n",
    "Let's statisticaly assesse this finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fbc01-6a6f-4106-9b7c-01454edae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "\n",
    "# Iterate over each column pair\n",
    "for column in numerical_columns :\n",
    "    if column != 'revenue':  \n",
    "\n",
    "        # Calculate the Pearson correlation coefficient and p-value\n",
    "        correlation_coefficient, p_value = spearmanr(grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df['revenue'],\n",
    "                                                     grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df[column])\n",
    "\n",
    "        # Interpret the results and print\n",
    "        significance = \"significant\" if p_value < 0.05 else \"no significant\"\n",
    "        print(f\"There is a {significance} correlation between the revenue' and the {column}.\")\n",
    "        print(f\"Correlation coefficient: {correlation_coefficient:.2f}\")\n",
    "        print('-' * 50)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7757ec-261b-40dd-9d3d-ea5d8ec65a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df,\n",
    "             x_vars=['purchases', 'unique_invoices', 'unique_streams', 'total_views'], \n",
    "             y_vars='revenue'\n",
    "            )\n",
    "plt.suptitle(\"Pairplot: Revenue vs Other Variables\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed884ba-8013-4d3d-bffc-6d34c43bb040",
   "metadata": {},
   "source": [
    "The independent variables are ['purchases', 'unique_invoices', 'unique_streams', 'total_views'] while the dependent variable is 'revenue'\n",
    "\n",
    "So to tackle the multicollinearity between predictors, we ll use Ridge regression, feature selection, and feature engineering to perform a regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606a065-45d8-4681-b150-f0c03ee7719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we construct a feauture\n",
    "\n",
    "df_reg = grouped_time_series_cleaned_from_IsolationForest_outliers_loaded_df\n",
    "\n",
    "# Feature Engineering: Create a new feature for engagement\n",
    "df_reg['engagement'] = df_reg['unique_streams'] + df_reg['total_views']\n",
    "\n",
    "# Prepare the data for Ridge regression\n",
    "X = df_reg[['purchases', 'unique_invoices', 'engagement']]\n",
    "y = df_reg['revenue']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # You may tune alpha using cross-validation\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print('Coefficients:', ridge_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0672f-61f2-41e1-83b1-1060a7c289cc",
   "metadata": {},
   "source": [
    "the results are bad so we will not use feature engenieering, instead we ll perform feature selection by iterating over the predictors and finding the best combination of predictors that provides the lowest Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b42d79-c184-4f47-a1f2-0980afa2b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the features and target variable\n",
    "X = df_reg[['purchases', 'unique_invoices', 'unique_streams', 'total_views']]\n",
    "y = df_reg['revenue']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize variables to track the best model\n",
    "best_mse = float('inf')\n",
    "best_features = None\n",
    "\n",
    "# Iterate through all combinations of predictors\n",
    "features = X.columns\n",
    "num_features = len(features)\n",
    "\n",
    "for i in range(1, num_features + 1):\n",
    "    for combination in itertools.combinations(features, i):\n",
    "        # Train the model with the selected features\n",
    "        model = RidgeCV(alphas=np.logspace(-6, 6, 13), store_cv_values=True)\n",
    "        model.fit(X_train[list(combination)], y_train)\n",
    "        \n",
    "        # Make predictions and calculate MSE\n",
    "        predictions = model.predict(X_test[list(combination)])\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "        # Check if this is the best model\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_features = combination\n",
    "\n",
    "# Output the best model results\n",
    "print(\"Best features:\", best_features)\n",
    "print(\"Best Mean Squared Error:\", best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2fa2c-2d77-4a4e-9526-c1356e466234",
   "metadata": {},
   "source": [
    "Using the best features identified during feature selection we build a simple linear regression model for clear interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bc91a-0d59-4b97-a1e3-03b56228ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and target variable\n",
    "selected_features = list(best_features)\n",
    "X = df_reg[selected_features]\n",
    "y = df_reg['revenue']\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579afb3d-9805-4007-b6e8-db87d0d34af8",
   "metadata": {},
   "source": [
    "The OLS regression results indicate a strong model with an R-squared of 0.813, suggesting that approximately 81.3% of the variance in monthly revenue across all country can be explained by our predictors: purchases, unique streams, and total views. \n",
    "\n",
    "The significant coefficient for purchases (p = 0.002) implies a positive relationship, while total views shows a significant negative impact (p = 0.046)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce3d8a-3910-4b45-8fce-7bfa073ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98ff99-1a65-4ffa-ba42-0d42e61af7d1",
   "metadata": {},
   "source": [
    "### Hypothesis 3: Seasonal Revenue Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e0344-2943-4eb0-9663-238e9c66ca3a",
   "metadata": {},
   "source": [
    "`Statement`: Monthly revenue exhibits seasonal trends, indicating certain months generate higher revenue than others.\n",
    "\n",
    "`Involved Fields`: year, month, price\n",
    "\n",
    "`Target` : The Monthly Revenue which is the total revenue generated in a given month for each country.\n",
    "\n",
    "`Business Metric` : Seasonal Revenue Trends: The key business metric to evaluate is whether monthly revenue varies significantly across different months of the year. Understanding these trends is vital for planning marketing efforts, budgeting, and staffing.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "`Data Aggregation`: Aggregate the data by year and month to calculate the total monthly revenue. This involves summing the price for all transactions that occur in that month for each year.\n",
    "The resulting dataset will have fields: year, month, and monthly_revenue.\n",
    "\n",
    "`Statistical Analysis` : \n",
    "- Perform a time series analysis to evaluate seasonal patterns in the monthly revenue data. This could involve using methods such as Seasonal Decomposition of Time Series (STL) to separate the seasonal component from the trend and residual components.\n",
    "- Conduct ANOVA tests to determine if there are statistically significant differences in monthly revenue across different months.\n",
    "\n",
    "`Visualizations`: Create line plots to visualize monthly revenue over multiple years. This can help identify seasonal patterns and trends, showing which months consistently generate higher revenue.\n",
    "Box plots can also be useful to display the distribution of monthly revenue for each month across different years, highlighting variations and potential outliers.\n",
    "\n",
    "`Insights Extraction`: Based on the results of the seasonal decomposition and statistical tests, draw conclusions about which months generate higher revenue and whether these patterns are consistent across years.\n",
    "Identify factors that might influence seasonal trends, such as holidays, special events, or seasonal marketing campaigns.\n",
    "\n",
    "`Considerations`: It's important to consider external factors that may affect revenue in certain months (e.g., economic conditions, industry trends, or competition). Understanding these factors can provide context for the observed seasonal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365521c7-5b72-4844-8614-61659485541a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe4a7e6-4772-4016-8001-db1bf7da92c9",
   "metadata": {},
   "source": [
    "### Hypothesis 4: Customer Purchase Behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b05eb5-2a83-4249-8b8b-53689247fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "we have to drop missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a126e1-3701-4c7b-a74a-4344e7c2f0bf",
   "metadata": {},
   "source": [
    "`Statement`: The average purchase price per customer varies by country.\n",
    "\n",
    "`Involved Fields`: country, customer_id, price\n",
    "\n",
    "`Target` : Average Purchase Price per Customer: This is calculated by summing the price of all transactions for each customer in a given country and dividing by the number of transactions they have made. The resulting measure will represent the average amount spent per customer.\n",
    "\n",
    "`Business Metric` : Variability in Average Purchase Price by Country: The key business metric to evaluate is whether the average purchase price per customer differs significantly across various countries. Understanding this variability can inform marketing strategies, pricing models, and customer engagement efforts.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "`Data Aggregation`: Group the data by country and customer_id to calculate the total purchase price for each customer. Then, for each country, compute the average purchase price per customer.\n",
    "The resulting dataset will have fields: country, customer_id, and average_purchase_price.\n",
    "\n",
    "`Statistical Analysis` : \n",
    "- Use descriptive statistics to summarize the average purchase price per customer for each country, including measures such as mean, median, and standard deviation.\n",
    "- Conduct ANOVA tests to determine if there are statistically significant differences in the average purchase price across different countries. This will help assess whether any observed variations are meaningful or due to random chance.\n",
    "\n",
    "`Visualizations`: Create bar charts or box plots to visualize the average purchase price per customer by country. This will provide an intuitive view of how purchase behavior differs across regions.\n",
    "Histograms can be useful to visualize the distribution of purchase prices within each country, highlighting patterns and potential outliers.\n",
    "\n",
    "`Insights Extraction`: Based on the results of the statistical analysis and visualizations, draw conclusions about the differences in average purchase prices per customer across countries.\n",
    "Identify potential factors influencing these differences, such as cultural preferences, economic conditions, or regional pricing strategies.\n",
    "\n",
    "`Considerations`: Keep in mind that customer demographics and purchasing power may vary by country, which could influence average spending behavior. Incorporating these factors into further analysis may yield more insights.\n",
    "Consider potential biases in the data, such as the number of transactions per customer or variations in service offerings across countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271bc7f-524f-4867-8e28-ce485cdcf17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
