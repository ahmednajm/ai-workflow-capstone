{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7e2db-919a-48c9-8810-42a628ffc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os, csv, time, uuid\n",
    "from datetime import date, datetime\n",
    "\n",
    "#LOG_DIR = os.path.join(os.path.dirname(__file__),'.','logs')\n",
    "LOG_DIR = 'logs'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "def update_train_log(\n",
    "    country: str,\n",
    "    date_range: str,\n",
    "    metric: str,\n",
    "    runtime: float,\n",
    "    version: str,\n",
    "    prefix: str,\n",
    "    note: str,\n",
    "    mode: str = 'prod',\n",
    "    test: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Append a new entry to the training log file with details about the model training run.\n",
    "\n",
    "    Parameters:\n",
    "    - country (str): The country associated with the training data.\n",
    "    - date_range (str): The date range of the training data.\n",
    "    - metric (str): Performance metric of the model (RMSE & MAPE).\n",
    "    - runtime (float): Runtime of the training process in seconds.\n",
    "    - version (str): Model version identifier.\n",
    "    - prefix (str): Log file prefix.\n",
    "    - note (str): Additional notes on the training run.\n",
    "    - mode (str): Mode of operation, e.g., 'production' or 'test'. **Default**: 'production'.\n",
    "    - test (bool): Flag indicating if this is a test run. **Default**: False.\n",
    "\n",
    "    Returns:\n",
    "    None. Writes data to a log file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Log Directory if it doesn't exist\n",
    "    if not os.path.isdir(LOG_DIR):\n",
    "        os.mkdir(LOG_DIR)\n",
    "\n",
    "    # Name the logfile and define its path\n",
    "    today = date.today()\n",
    "    train_logfile = os.path.join(LOG_DIR, f\"{mode}-trained_on_{today.month}_{today.year}.log\")\n",
    "\n",
    "    # Define the header\n",
    "    header = ['unique_id', 'timestamp', 'date_range', 'country', 'metric', 'model_version', 'runtime', 'mode', 'note']\n",
    "    write_header = False\n",
    "\n",
    "    # Write the header if needed\n",
    "    if not os.path.exists(train_logfile):\n",
    "        write_header = True\n",
    "\n",
    "    # Get the current timestamp\n",
    "    current_timestamp = datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # Generate a random UUID\n",
    "    unique_id = uuid.uuid4()\n",
    "    unique_id = str(unique_id)[:13]\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(train_logfile, mode='a+', newline='') as csvfile: \n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "        # Write the header if needed\n",
    "        if write_header:\n",
    "            writer.writerow(header)\n",
    "\n",
    "        # Prepare the row for writing\n",
    "        to_write = map(str, [unique_id, current_timestamp, date_range, country, metric, version, runtime, mode, note])\n",
    "\n",
    "        # Write the row to the log file\n",
    "        writer.writerow(to_write)\n",
    "\n",
    "    print(f\"\\nLog entry saved to {train_logfile}\")\n",
    "\n",
    "\n",
    "def update_predict_log(country: str, target_date: str, y_pred: str, y_proba: list, \n",
    "                       runtime: str, version: str, mode: str) -> None:\n",
    "    \"\"\"\n",
    "    Update the prediction log file with the prediction results.\n",
    "\n",
    "    Parameters:\n",
    "    - country (str): The target country for the prediction.\n",
    "    - target_date (str): The date of the prediction.\n",
    "    - y_pred (str): The predicted value.\n",
    "    - y_proba (list): The predicted probabilities.\n",
    "    - runtime (str): The runtime of the prediction.\n",
    "    - version (str): The version of the model used.\n",
    "    - mode (str): The mode of prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Name the logfile and define its path\n",
    "    today = date.today()\n",
    "    predict_logfile = os.path.join(LOG_DIR, f\"{mode}-predicted_on_{today.month}_{today.year}.log\")\n",
    "    \n",
    "    # Get the current timestamp\n",
    "    current_timestamp = datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # Generate a random UUID\n",
    "    unique_id = str(uuid.uuid4())[:13]\n",
    "    \n",
    "    # Define the header\n",
    "    header = ['unique_id', 'timestamp', 'mode', 'country', 'target_date', 'y_pred', 'y_proba', 'model_version', 'runtime']\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(predict_logfile, 'a+', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        \n",
    "        # Write the header if needed\n",
    "        if os.path.getsize(predict_logfile) == 0:  # Check if the file is empty\n",
    "            writer.writerow(header)\n",
    "            \n",
    "        to_write = map(str, [unique_id, current_timestamp, mode, country, target_date, y_pred, y_proba, version, runtime])\n",
    "        writer.writerow(to_write)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f30f11-3303-46f8-bfc8-6a3e5dd439bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## model specific variables (iterate the version and note with each change)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m NOTE      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom forest model for time-series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs-train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#MODEL_DIR = os.path.join(os.path.dirname(__file__), '.', \"models\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m MODEL_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries \n",
    "import os, re, time, joblib\n",
    "from typing import Tuple\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from data_module import load_json_data, time_series_df\n",
    "from cleaning_module import data_cleaning_pipeline\n",
    "from logger import update_predict_log, update_train_log\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## model specific variables (iterate the version and note with each change)\n",
    "NOTE      = \"random forest model for time-series\"\n",
    "DATA_DIR = os.path.join(os.path.dirname(__file__), '.', 'data', 'cs-train')\n",
    "#MODEL_DIR = os.path.join(os.path.dirname(__file__), '.', \"models\")\n",
    "MODEL_DIR = \"models\"\n",
    "version   = 'v4'\n",
    "mode      = 'prod'\n",
    "\n",
    "## Random Forest Regressor model specific variables \n",
    "prefix           = 'sl'\n",
    "#country          = 'all_countries'\n",
    "#model_scaler     = StandardScaler()\n",
    "#model            = RandomForestRegressor( random_state = 42)\n",
    "model_param_grid = { 'model__n_estimators' : [90, 100, 110, 120, 130] ,\n",
    "                     'model__max_depth'    : [None, 5, 10, 15]        ,\n",
    "                     'model__criterion'    : ['squared_error']        }\n",
    "\n",
    "\n",
    "# start timer for runtime\n",
    "loading_df_time_start = time.time()\n",
    "\n",
    "## loading the dataframe as loaded_df\n",
    "loaded_df_original = load_json_data(DATA_DIR)\n",
    "print(f\"\\n... Data injested\\n\")\n",
    "\n",
    "## Rows count\n",
    "print(f'\\nInjested data contains initialy {len(loaded_df_original):,.0f} entries\\n')\n",
    "\n",
    "# calculate runtime duration\n",
    "m, s = divmod( time.time() - loading_df_time_start , 60)\n",
    "h, m = divmod(m, 60)\n",
    "loading_df_runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "print(f'\\nData injestion runtime : {loading_df_runtime}\\n')\n",
    "\n",
    "# start timer for runtime\n",
    "cleaning_time_start = time.time()\n",
    "\n",
    "# clean the dataframe\n",
    "loaded_df = data_cleaning_pipeline(loaded_df_original)\n",
    "\n",
    "# calculate runtime duration\n",
    "m, s = divmod( time.time() - cleaning_time_start , 60)\n",
    "h, m = divmod(m, 60)\n",
    "cleaning_runtime = \"%03d:%02d:%02d\"%(h, m, s)\n",
    "print(f'\\nCleaning runtime : {cleaning_runtime}\\n')\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, country: str, training: bool) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Engineer features for each day to predict the sum of revenue for the next 30 days.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with 'date', 'revenue', 'purchases', and 'total_views'.\n",
    "    country (str): Country to filter data.\n",
    "    training (bool): If True, trims the last 30 days of data; otherwise, returns all data.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.Series, pd.Series]: \n",
    "        - DataFrame with engineered features.\n",
    "        - Target values (sum of next 30 days' revenue).\n",
    "        - Dates corresponding to each feature row.\n",
    "    \"\"\"\n",
    "    \n",
    "    engineer_features_time_start = time.time()\n",
    "\n",
    "    ts_df = time_series_df(df, country=country)\n",
    "    ts_df = ts_df[['date', 'revenue', 'purchases', 'total_views']]\n",
    "    ts_df['date'] = pd.to_datetime(ts_df['date'], errors='coerce')\n",
    "\n",
    "    # Initialize dictionaries to store features and target values\n",
    "    eng_features = defaultdict(list)\n",
    "    y = []\n",
    "\n",
    "    # Define the look-back periods (in days) for feature engineering\n",
    "    previous_days = [7, 14, 28, 70]\n",
    "\n",
    "    # Calculate rolling sums for revenue for each period and shift to align with target\n",
    "    for num_days in previous_days:\n",
    "        ts_df[f'revenue_{num_days}'] = ts_df['revenue'].rolling(window=num_days, min_periods=1).sum().shift(1)\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in ts_df.iterrows():\n",
    "        current_date = row['date']\n",
    "\n",
    "        # Append engineered features\n",
    "        for num_days in previous_days:\n",
    "            eng_features[f'previous_{num_days}'].append(row[f'revenue_{num_days}'])\n",
    "\n",
    "        # Target: Sum revenue for the next 30 days\n",
    "        target_sum = ts_df[(ts_df['date'] >= current_date) & (ts_df['date'] < current_date + pd.Timedelta(days=30))]['revenue'].sum()\n",
    "        y.append(target_sum)\n",
    "        \n",
    "        # Previous year revenue for trend analysis\n",
    "        prev_year_start = current_date - pd.DateOffset(years=1)\n",
    "        prev_year_revenue = ts_df[(ts_df['date'] >= prev_year_start) & (ts_df['date'] < prev_year_start + pd.DateOffset(days=30))]['revenue'].sum()\n",
    "        eng_features['previous_year'].append(prev_year_revenue)\n",
    "        \n",
    "        # Non-revenue features: Average invoices and views over the last 30 days\n",
    "        recent_data = ts_df[(ts_df['date'] >= current_date - pd.Timedelta(days=30)) & (ts_df['date'] < current_date)]\n",
    "        eng_features['recent_views'].append(recent_data['total_views'].mean() if not recent_data.empty else 0)\n",
    "    \n",
    "    # Convert the features dictionary to a DataFrame\n",
    "    X = pd.DataFrame(eng_features)\n",
    "    y = pd.Series(y, name='target')\n",
    "    dates = ts_df['date']\n",
    "\n",
    "    # Remove rows with all zeros (in cases where no data exists for look-back periods)\n",
    "    X = X[(X != 0).any(axis=1)]\n",
    "    y = y[X.index]\n",
    "    dates = dates[X.index]\n",
    "\n",
    "    # If training, exclude the last 30 days to ensure target reliability\n",
    "    if training:\n",
    "        X = X.iloc[:-30]\n",
    "        y = y.iloc[:-30]\n",
    "        dates = dates.iloc[:-30]\n",
    "\n",
    "    # Reset index for neatness\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    dates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Calculate runtime duration\n",
    "    m, s = divmod(time.time() - engineer_features_time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    engineer_features_runtime = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    \n",
    "    return X, y, dates\n",
    "\n",
    "\n",
    "def perform_training(df: pd.DataFrame, country: str, prefix: str, version: str,\n",
    "                    model, model_param_grid: dict, model_scaler,\n",
    "                    training: bool, test: bool) -> str:\n",
    "    \"\"\"\n",
    "    Trains a model on the given dataset, tunes hyperparameters, evaluates performance, and saves the model.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input data for training and testing.\n",
    "    - country (str): Country for which the model is being trained.\n",
    "    - prefix (str): Prefix for saved model file name.\n",
    "    - version (str): Version identifier for the model.\n",
    "    - model: Model to be trained (e.g., RandomForestRegressor).\n",
    "    - model_param_grid (dict): Hyperparameters grid for tuning the model.\n",
    "    - model_scaler: Scaler to normalize the data.\n",
    "    - training (bool): Flag for training data usage.\n",
    "    - test (bool): Flag for test mode; saves the model with \"test\" prefix if True. **Default**: False.\n",
    "\n",
    "    Returns:\n",
    "    - str: Name of the model used.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n... Perform training on train set\")\n",
    "\n",
    "    # start timer for runtime\n",
    "    perform_training_time_start = time.time()\n",
    "    \n",
    "    # prepare the data\n",
    "    X, y, dates = engineer_features(df, country, training)\n",
    "    \n",
    "    # Execute this block only if model is RandomForestRegressor\n",
    "    if isinstance(model, RandomForestRegressor):\n",
    "        X = X.dropna()  \n",
    "        y = y[X.index]\n",
    "        dates = dates[X.index]\n",
    "    \n",
    "    # Create Test Subset of Data (if in Test Mode)\n",
    "    if test:\n",
    "        subset = X.sample(frac=0.30, replace=False, random_state=42).index\n",
    "        dates = dates.loc[subset]\n",
    "        X = X.loc[subset]\n",
    "        y = y.loc[subset]\n",
    "\n",
    "    # Define the date range for logging\n",
    "    max_date = dates.iloc[-1].strftime('%Y-%m-%d')\n",
    "    min_date = dates.iloc[0].strftime('%Y-%m-%d')\n",
    "    date_range = f\"{min_date}:{max_date}\"\n",
    "    \n",
    "    # Perform a train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.20, shuffle=True)\n",
    "\n",
    "    # Create a pipeline with scaling and a random forest model\n",
    "    pipe = Pipeline([('model_scaler', model_scaler), ('model', model)])\n",
    "    \n",
    "    # Tune the hyperparameter\n",
    "    print(\"\\n... Tuning the model hyperparameters \")\n",
    "    grid = GridSearchCV(pipe, param_grid=model_param_grid, cv=5, n_jobs=2)\n",
    "\n",
    "    # Fit the model on train set\n",
    "    try:\n",
    "        grid.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training on train set: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Make predictions on test set \n",
    "    y_pred = grid.predict(X_test)\n",
    "    ## Evaluate the model on test set\n",
    "    # Calculate RMSE\n",
    "    eval_rmse = round(mean_squared_error(y_test, y_pred)**0.5)\n",
    "    # Calculate MAPE\n",
    "    eval_mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    # Define the date range for logging\n",
    "    eval_metrics = f\"[RMSE={eval_rmse},MAPE={eval_mape:.1f}%]\"\n",
    "\n",
    "    # Retrain using all data\n",
    "    print(\"\\n... Retraining model on all data\")\n",
    "    try:\n",
    "        grid.fit(X, y)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model retraining on all data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Best model\n",
    "    fitted_model = grid.best_estimator_\n",
    "    \n",
    "    # make the model name more system compatible and file-friendly when saving the model\n",
    "    model_name = fitted_model.named_steps['model'].__class__.__name__\n",
    "\n",
    "    # Define the file path \n",
    "    if test:\n",
    "        saved_model = os.path.join(MODEL_DIR, f\"test-{country}-{model_name}-{version}.joblib\")\n",
    "    else:\n",
    "        saved_model = os.path.join(MODEL_DIR, f\"{prefix}-{country}-{model_name}-{version}.joblib\")\n",
    "\n",
    "    # Save the fitted model\n",
    "    print(f\"\\n... Saving model version as {prefix} : {saved_model}\\n\")\n",
    "    joblib.dump(fitted_model, saved_model)\n",
    "\n",
    "\n",
    "    # Calculate runtime duration\n",
    "    m, s = divmod(time.time() - perform_training_time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    perform_training_runtime = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "    # update training log\n",
    "    update_train_log(country, date_range, eval_metrics, perform_training_runtime, version, prefix, test=False, note=NOTE)\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "\n",
    "def model_train(df: pd.DataFrame, country: str, prefix: str, version: str,\n",
    "                model, model_param_grid: dict, model_scaler, \n",
    "                training: bool = True, test: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Train the model with the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataset for training.\n",
    "    - country (str): Target country for the model.\n",
    "    - prefix (str): Prefix for naming saved model files.\n",
    "    - version (str): Version number for the model.\n",
    "    - model: The machine learning model to be trained.\n",
    "    - model_param_grid (dict): Hyperparameter grid for tuning.\n",
    "    - model_scaler: Scaler for data normalization.\n",
    "    - training (bool): Flag indicating training mode. **Default**: True.\n",
    "    - test (bool): Flag for test mode. If True, saves model with a \"test\" prefix. **Default**: False.\n",
    "\n",
    "    Returns:\n",
    "    - str: Model name after training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start timer for runtime\n",
    "    model_train_time_start = time.time()\n",
    "\n",
    "    # Create model directory \n",
    "    if not os.path.isdir(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "        print(f\"Created model directory: {MODEL_DIR}\")\n",
    "\n",
    "    # Test Mode Notification\n",
    "    if test:\n",
    "        print(\"...... testing\")\n",
    "        print(\"...... subseting data\")\n",
    "\n",
    "    model_name = perform_training(df, country, prefix, version, model, model_param_grid, model_scaler, training, test)    \n",
    "    print(f'\\nModel \"{prefix}-{country}-{model_name}-{version}\" trained')\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate runtime duration\n",
    "    m, s = divmod(time.time() - model_train_time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    model_train_runtime = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    print(f'\\nModel training runtime : {model_train_runtime}\\n')\n",
    "\n",
    "\n",
    "\n",
    "def model_load(country: str, training: bool, df: pd.DataFrame, prefix: str = 'prod') -> tuple:\n",
    "    \"\"\"\n",
    "    Loads trained models for a specified country and prepares features for predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - country (str): The target country for which models are loaded.\n",
    "    - training (bool): Flag indicating if the data is for training or evaluation.\n",
    "    - df (DataFrame): The dataset from which features are engineered.\n",
    "    - prefix (str): The prefix used to identify models.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary of models keyed by model name.\n",
    "    - dict: A dictionary containing features `X`, target `y`, and `dates`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start timer for runtime\n",
    "    model_load_time_start = time.time()\n",
    "\n",
    "    # Model directory path\n",
    "    model_dir = \"./models\"\n",
    "\n",
    "    # Initialize return values\n",
    "    data_dict = {}\n",
    "    models_dict = {}\n",
    "\n",
    "    # Retrieve all model filenames matching the specified prefix and country\n",
    "    models = [filename for filename in os.listdir(model_dir) if prefix in filename and country in filename]\n",
    "\n",
    "    # Check if any models were found\n",
    "    if not models:\n",
    "        print(f\"Models starting with the prefix '{prefix}' for the '{country}' country cannot be found! Had you trained it?\")\n",
    "        return models_dict, data_dict  # Return empty dictionaries\n",
    "\n",
    "    # Load models into a dictionary, keyed by model name without file extension\n",
    "    models_dict = {model.replace('.joblib', ''): joblib.load(os.path.join(model_dir, model)) for model in models}\n",
    "\n",
    "    # Engineer features and target variable from the provided DataFrame\n",
    "    X, y, dates = engineer_features(df, country, training)\n",
    "\n",
    "    # Convert dates to string format for consistency\n",
    "    dates = pd.to_datetime(dates).dt.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "    # Compile the data into a dictionary for easy access\n",
    "    data_dict = {\"X\": X, \"y\": y, \"dates\": dates}\n",
    "\n",
    "    # Calculate runtime duration\n",
    "    m, s = divmod(time.time() - model_load_time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    model_load_runtime = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    print(f'Loading model runtime : {model_load_runtime}')\n",
    "\n",
    "    return models_dict, data_dict\n",
    "\n",
    "\n",
    "\n",
    "def nearest_date(items: list, pivot: date) -> date:\n",
    "    \"\"\"Find the nearest date to the given pivot date.\"\"\"\n",
    "    if not items:\n",
    "        raise ValueError(\"ERROR: items list is empty.\")\n",
    "    return min(items, key=lambda x: abs(date.fromisoformat(x) - pivot))\n",
    "\n",
    "\n",
    "\n",
    "def model_predict(df: pd.DataFrame, country: str, year: int, month: int, day: int, \n",
    "                  mode: str = 'prod', models_dict: dict = None, training: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model for a specified country and date.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataset for prediction.\n",
    "    - country (str): The target country for the prediction.\n",
    "    - year (int): The year of the target date.\n",
    "    - month (int): The month of the target date.\n",
    "    - day (int): The day of the target date.\n",
    "    - mode (str): The mode for prediction (e.g., 'prod'). **Default**: 'prod'.\n",
    "    - models_dict (dict): A dictionary of models keyed by model name. **Default**: None.\n",
    "    - training (bool): Flag indicating if the data is for training or evaluation. **Default**: False.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing predicted values and probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Start timer for runtime\n",
    "    model_predict_time_start = time.time()\n",
    "    \n",
    "    ## Load model if needed\n",
    "    print(f\"\\n... Loading models\")    \n",
    "    if models_dict is None:\n",
    "        models_dict, data_dict = model_load(country, training, df, prefix='sl')\n",
    "\n",
    "    # Input checks   \n",
    "    if not any(f'{country}' in key for key in models_dict.keys()):\n",
    "        raise Exception(f\"\\nERROR (model_predict) - any model for country '{country}' could not be found\")\n",
    "\n",
    "    # Finding the model with the latest version\n",
    "    latest_model_key = max(models_dict.keys(), key=lambda k: int(k.split('-')[-1][1:]))\n",
    "    latest_model = models_dict[latest_model_key]\n",
    "    print(f\"\\nLatest Model used for {country}: {latest_model_key}\")    \n",
    "\n",
    "    # Validate Date Components\n",
    "    for d in [year, month, day]:\n",
    "        if re.search(r\"\\D\", str(d)):  \n",
    "            raise Exception(\"ERROR (model_predict) - invalid year, month, or day\")\n",
    "        \n",
    "    ## Load data\n",
    "    data = data_dict\n",
    "\n",
    "    # Convert dates to Datetime Format\n",
    "    dates = pd.to_datetime(data['dates'])\n",
    "\n",
    "    # Check the target date\n",
    "    target_date = f\"{year}-{str(month).zfill(2)}-{str(day).zfill(2)}\"\n",
    "    print(f\"\\n... Checking if {target_date} is in the range.\")\n",
    "\n",
    "    # Validate Target Date and Find Nearest Date if Out of Range\n",
    "    if target_date not in dates.astype(str).values:  # Convert to string for comparison\n",
    "        print(f\"ERROR (model_predict) - date {target_date} not in range [ {data['dates'].iloc[0]} - {data['dates'].iloc[-1]} ]\")\n",
    "        target_date = nearest_date(data['dates'], date.fromisoformat(target_date))\n",
    "        print(f\"Nearest target date is {target_date}\")\n",
    "    else:\n",
    "        print(\"Target date is in the range.\")\n",
    "\n",
    "    # Get the index of the target_date\n",
    "    target_date_indx = dates.get_loc(target_date)  # Correct way to get the index\n",
    "\n",
    "    # Query the corresponding row\n",
    "    query = data['X'].iloc[[target_date_indx]]  \n",
    "\n",
    "    ## Make prediction\n",
    "    y_pred = latest_model.predict(query)\n",
    "\n",
    "    ## Add a probability to the prediction \n",
    "    y_proba = None\n",
    "    if 'predict_proba' in dir(latest_model) and getattr(latest_model, 'probability', False):\n",
    "        y_proba = latest_model.predict_proba(query)\n",
    "\n",
    "    # Calculate runtime duration\n",
    "    m, s = divmod(time.time() - model_predict_time_start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    model_predict_runtime = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    print(f'\\nPredicting runtime: {model_predict_runtime}\\n')\n",
    "\n",
    "    # Update predict log\n",
    "    update_predict_log(country, target_date, f'{y_pred[0]:,.0f}', y_proba, model_predict_runtime, latest_model_key, mode)\n",
    "\n",
    "    print(f'The expected revenue over the next 30 days is {y_pred[0]:,.0f} £')\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    from argparse import ArgumentParser\n",
    "    ap = ArgumentParser()\n",
    "    \n",
    "    ap.add_argument('-t', '--training', choices=['dev', 'prod'], default='dev',\n",
    "                    help='Train either a development or production model. Omitting this implies loading an already-trained model')\n",
    "    \n",
    "    ap.add_argument('-m', '--model', choices=['rf', 'et'], default='rf',\n",
    "                    help='(rf) RandomForestRegressor or (et) ExtraTreesRegressor (default)')\n",
    "    \n",
    "    ap.add_argument('-s', '--scaler', choices=['ss', 'rs'], default='ss',\n",
    "                    help='(ss) StandardScaler or (rs) RobustScaler (default)')\n",
    "    \n",
    "    ap.add_argument('-c', '--country', default='all_countries',\n",
    "                    help=\"The country to predict revenue for (default: 'all_countries')\")\n",
    "    \n",
    "    args = ap.parse_args()\n",
    "    \n",
    "    train  = args.training\n",
    "    model  = RandomForestRegressor(random_state = 42) if args.model == 'rf' else ExtraTreesRegressor(random_state = 42)\n",
    "    scaler = StandardScaler() if args.scaler == 'ss' else RobustScaler()\n",
    "    \n",
    "    if train == 'dev':\n",
    "        # train the model - Development\n",
    "        print(\"TRAINING MODELS - DEVELOPMENT\")\n",
    "        model_train(loaded_df, country, prefix, version, model, model_param_grid, model_scaler, training= True, test=True)\n",
    "    elif train == 'prod':\n",
    "        # train the model - Production\n",
    "        print(\"TRAINING MODELS - PRODUCTION\")\n",
    "        model_train(loaded_df, country, prefix, version, model, model_param_grid, model_scaler, training= True, test=False)\n",
    "    else:\n",
    "        # load the model\n",
    "        print(\"LOADING MODELS\")\n",
    "        all_data, all_models = model_load(training=False, country=country)\n",
    "        model_load(country, df, training=False, prefix='prod') \n",
    "        print(\"... models loaded: \", \",\".join(all_models.keys()))\n",
    "\n",
    "    # Predicting for:\n",
    "    year = '2019'\n",
    "    month = '06'\n",
    "    day = '05'\n",
    "    \n",
    "    result = model_predict(loaded_df, country, year, month, day, mode='prod', models_dict= None, training=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d5181-6be9-420e-8de1-d2c6be886798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
